{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing SQLite Database**\n",
    "After doing a bunch of data preparation in the previous notebooks, I'm ready to create a SQLite database with all of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook. \n",
    "\n",
    "I'll start by configuring the kernel that's running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thubbard/Documents/personal/programming/pax-pal-2025/experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the cwd\n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "\n",
    "# Project-specific imports \n",
    "import utils.openai as openai_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "First off: I'll load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/6dyp2b_d72z_d772nvm0y6tc0000gn/T/ipykernel_55797/1373856853.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  playable_games_df = pd.read_json(\"data/final_enriched_games_data.json\")\n",
      "/var/folders/p0/6dyp2b_d72z_d772nvm0y6tc0000gn/T/ipykernel_55797/1373856853.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  playable_games_df = pd.read_json(\"data/final_enriched_games_data.json\")\n",
      "/var/folders/p0/6dyp2b_d72z_d772nvm0y6tc0000gn/T/ipykernel_55797/1373856853.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  playable_games_df = pd.read_json(\"data/final_enriched_games_data.json\")\n"
     ]
    }
   ],
   "source": [
    "# Load in the playable_games_df\n",
    "playable_games_df = pd.read_json(\"data/final_enriched_games_data.json\")\n",
    "\n",
    "# If any rows have an empty `description_texts` / `genres_and_tags`, drop them\n",
    "playable_games_df = playable_games_df[\n",
    "    playable_games_df[\"description_texts\"].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0\n",
    "    )\n",
    "    & playable_games_df[\"genres_and_tags\"].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many games do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(playable_games_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Text\n",
    "Before I actually create the SQLite database, I'm going to embed all of the relevant text. Specifically, I'll embed: \n",
    "\n",
    "- The average of each of the texts within `description_texts`\n",
    "- The `snappy_summary`\n",
    "- A comma-separated string of everything in `genres_and_tags`\n",
    "\n",
    "I'll start by grabbing all of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text_to_embed_df\n",
    "text_to_embed_df_records = []\n",
    "for row in playable_games_df.itertuples():\n",
    "    # Find the longest description text\n",
    "    longest_description = max(\n",
    "        [desc_dict.get(\"text\", \"\") for desc_dict in row.description_texts],\n",
    "        key=len,\n",
    "        default=\"\",\n",
    "    )\n",
    "\n",
    "    # Create a single combined text\n",
    "    combined_text = f\"{row.name} {row.snappy_summary} {', '.join(row.genres_and_tags)} {longest_description}\"\n",
    "\n",
    "    text_to_embed_df_records.append(\n",
    "        {\"game_id\": row.id, \"emb_type\": \"combined\", \"text\": combined_text}\n",
    "    )\n",
    "text_to_embed_df = pd.DataFrame(text_to_embed_df_records).dropna(subset=[\"text\"])\n",
    "\n",
    "# Drop anything where the text is not a string\n",
    "text_to_embed_df = text_to_embed_df[\n",
    "    text_to_embed_df[\"text\"].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll embed everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = openai_utils.generate_embeddings_for_texts(\n",
    "    text_list=text_to_embed_df[\"text\"].tolist(), show_progress=True\n",
    ")\n",
    "\n",
    "# Add the embeddings to the dataframe\n",
    "embs_df = text_to_embed_df.copy()\n",
    "embs_df[\"emb\"] = embs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Embedding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_id     object\n",
      "emb_type    object\n",
      "text        object\n",
      "emb         object\n",
      "dtype: object\n",
      "Found similar games for 146 games\n",
      "Example - Top similar games for ea4c3274bf51:\n",
      "  31c4a8554ad9: similarity = 0.5651\n",
      "  31970e15ae31: similarity = 0.5373\n",
      "  19635cb48621: similarity = 0.5347\n",
      "  0084af272678: similarity = 0.5155\n",
      "  4f865f58714a: similarity = 0.5143\n",
      "  3f83464714d6: similarity = 0.5121\n",
      "\n",
      "Similar games DataFrame shape: (146, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the data types in embs_df\n",
    "print(embs_df.dtypes)\n",
    "\n",
    "# Convert embeddings from string to numpy arrays if needed\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "\n",
    "# Set the number of top similar games to retrieve\n",
    "n = 6\n",
    "\n",
    "# Make sure embeddings are numpy arrays\n",
    "if isinstance(embs_df[\"emb\"].iloc[0], str):\n",
    "    embs_df[\"emb\"] = embs_df[\"emb\"].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "# Extract game IDs and embeddings\n",
    "game_ids = embs_df[\"game_id\"].tolist()\n",
    "embeddings = np.array(embs_df[\"emb\"].tolist())\n",
    "\n",
    "# Calculate pairwise cosine similarity\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create a DataFrame to store the similarity matrix with game_ids as index and columns\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=game_ids, columns=game_ids)\n",
    "\n",
    "# For each game, find the top n most similar games (excluding itself)\n",
    "top_similar_games = {}\n",
    "for i, game_id in enumerate(game_ids):\n",
    "    # Get similarities for this game (excluding itself)\n",
    "    similarities = [\n",
    "        (game_ids[j], similarity_matrix[i, j])\n",
    "        for j in range(len(game_ids))\n",
    "        if game_ids[j] != game_id\n",
    "    ]\n",
    "\n",
    "    # Sort by similarity score (descending) and take top n\n",
    "    top_n = heapq.nlargest(n, similarities, key=lambda x: x[1])\n",
    "\n",
    "    # Store as {game_id: [(similar_game_id, similarity_score), ...]}\n",
    "    top_similar_games[game_id] = top_n\n",
    "\n",
    "# Preview the results\n",
    "print(f\"Found similar games for {len(top_similar_games)} games\")\n",
    "sample_game_id = list(top_similar_games.keys())[0]\n",
    "print(f\"Example - Top similar games for {sample_game_id}:\")\n",
    "for similar_game_id, score in top_similar_games[sample_game_id]:\n",
    "    print(f\"  {similar_game_id}: similarity = {score:.4f}\")\n",
    "\n",
    "# Create a DataFrame with the similar games information\n",
    "similar_games_records = []\n",
    "for game_id, similar_games in top_similar_games.items():\n",
    "    similar_games_dict = {\n",
    "        \"game_id\": game_id,\n",
    "        \"similar_game_ids\": [sg[0] for sg in similar_games],\n",
    "        \"similarity_scores\": [float(sg[1]) for sg in similar_games],\n",
    "    }\n",
    "    similar_games_records.append(similar_games_dict)\n",
    "\n",
    "similar_games_df = pd.DataFrame(similar_games_records)\n",
    "print(f\"\\nSimilar games DataFrame shape: {similar_games_df.shape}\")\n",
    "\n",
    "# Create a dictionary mapping each game_id to its top similar games\n",
    "similar_games_dict = {\n",
    "    row[\"game_id\"]: row[\"similar_game_ids\"] for _, row in similar_games_df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating SQLite Database\n",
    "Next up: I'm going to save all of the data in a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database at data/database.sqlite\n",
      "Database created successfully with 148 games and 148 embeddings ðŸŽ®\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import sqlite_vec  # vec0 helper\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Helper â”€ execute_many with retry/back-off to dodge the \"database is locked\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _execute_many_with_retry(\n",
    "    cursor,\n",
    "    sql: str,\n",
    "    data,\n",
    "    *,\n",
    "    max_attempts: int = 5,\n",
    "    initial_wait: float = 0.3,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run `cursor.executemany(sql, data)` with exponential back-off so that the\n",
    "    notebook doesn't crash when another process (or stray connection from a\n",
    "    previous cell) is holding a write-lock on the SQLite file.\n",
    "\n",
    "    Any non-locking OperationalError is re-raised immediately.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            cursor.executemany(sql, data)\n",
    "            return  # âœ… success\n",
    "        except sqlite3.OperationalError as exc:\n",
    "            # Only retry if it's really a lock issue\n",
    "            msg = str(exc).lower()\n",
    "            if \"database is locked\" not in msg:\n",
    "                raise\n",
    "\n",
    "            wait_time = initial_wait * (2**attempt)\n",
    "            print(\n",
    "                f\"SQLite is locked; retrying in {wait_time:.2f}s ({attempt+1}/{max_attempts})\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    # all retries exhausted\n",
    "    raise RuntimeError(\n",
    "        f\"Could not run query after {max_attempts} attempts; still locked.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Connection set-up\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "db_path = \"data/database.sqlite\"\n",
    "\n",
    "# Delete the database if it already exists to avoid schema conflicts\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(f\"Removed existing database at {db_path}\")\n",
    "\n",
    "with sqlite3.connect(\n",
    "    db_path, timeout=30.0, isolation_level=None, check_same_thread=False\n",
    ") as conn:\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)  # load vec0 extension\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Use default journal mode instead of WAL\n",
    "    cursor.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Schema\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cursor.executescript(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS games (\n",
    "            id                TEXT PRIMARY KEY,\n",
    "            name              TEXT,\n",
    "            snappy_summary    TEXT,\n",
    "            description_texts TEXT,\n",
    "            platforms         TEXT,\n",
    "            developer         TEXT,\n",
    "            exhibitor         TEXT,\n",
    "            booth_number      REAL,\n",
    "            header_image_url  TEXT,\n",
    "            steam_link        TEXT,\n",
    "            genres_and_tags   TEXT,\n",
    "            media             TEXT,\n",
    "            released          REAL,\n",
    "            release_time      TEXT,\n",
    "            links             TEXT,\n",
    "            similar_games     TEXT\n",
    "        );\n",
    "\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS game_embs\n",
    "        USING vec0(\n",
    "            game_id TEXT PRIMARY KEY,\n",
    "            vector  FLOAT[1536]\n",
    "        );\n",
    "        \n",
    "        -- Full-text search index on human-readable text\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS games_fts\n",
    "        USING fts5(\n",
    "            id UNINDEXED,\n",
    "            text\n",
    "        );\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1. Upsert games\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    game_records = [\n",
    "        (\n",
    "            row.id,\n",
    "            row.name,\n",
    "            getattr(row, \"snappy_summary\", None),\n",
    "            json.dumps(row.description_texts),\n",
    "            json.dumps(row.platforms),\n",
    "            row.developer,\n",
    "            row.exhibitor,\n",
    "            row.booth_number,\n",
    "            row.header_image_url,\n",
    "            row.steam_link,\n",
    "            json.dumps(row.genres_and_tags),\n",
    "            json.dumps(row.media),\n",
    "            row.released,\n",
    "            row.release_time,\n",
    "            json.dumps(getattr(row, \"links\", None)),\n",
    "            json.dumps(similar_games_dict.get(row.id, [])),\n",
    "        )\n",
    "        for row in playable_games_df.itertuples()\n",
    "    ]\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO games (\n",
    "            id, name, snappy_summary, description_texts, platforms,\n",
    "            developer, exhibitor, booth_number, header_image_url, steam_link,\n",
    "            genres_and_tags, media, released, release_time, links, similar_games\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        game_records,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2. Upsert embeddings (avoid UNIQUE constraint conflicts)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # clear out any stale rows\n",
    "    cursor.execute(\"DELETE FROM game_embs\")\n",
    "\n",
    "    # drop any accidental duplicates in the DataFrame\n",
    "    deduped = embs_df.drop_duplicates(subset=\"game_id\", keep=\"last\")\n",
    "    emb_records = [\n",
    "        (row[\"game_id\"], json.dumps(row[\"emb\"])) for _, row in deduped.iterrows()\n",
    "    ]\n",
    "\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"INSERT OR REPLACE INTO game_embs (game_id, vector) VALUES (?, ?)\",\n",
    "        emb_records,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3. Populate full-text search index\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cursor.execute(\"DELETE FROM games_fts\")\n",
    "\n",
    "    # Prepare records for FTS table\n",
    "    fts_records = []\n",
    "    for _, row in embs_df.iterrows():\n",
    "        fts_records.append((row[\"game_id\"], row[\"text\"]))\n",
    "\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"\"\"\n",
    "        INSERT INTO games_fts (id, text)\n",
    "        VALUES (?, ?)\n",
    "        \"\"\",\n",
    "        fts_records,\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Database created successfully with \"\n",
    "    f\"{len(playable_games_df)} games and {len(embs_df)} embeddings ðŸŽ®\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec\n",
    "import struct\n",
    "import json  # For potentially pretty-printing game details if needed\n",
    "\n",
    "# Database path\n",
    "db_path = \"data/database.sqlite\"\n",
    "\n",
    "# Dimension of your embeddings (must match what you stored)\n",
    "# From your schema: CREATE VIRTUAL TABLE ... vector FLOAT[1536]\n",
    "embedding_dim = 1536\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Connect to the database\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    # Enable extension loading and load sqlite-vec\n",
    "    # Although not strictly needed for reading blobs,\n",
    "    # it's good practice if you might do vector operations later.\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)\n",
    "    conn.enable_load_extension(False)  # Disable after loading\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query to get top 3 games by booth number (desc) and their embeddings\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        g.id,\n",
    "        g.name,\n",
    "        g.booth_number,\n",
    "        ge.vector  -- This will be a BLOB\n",
    "    FROM\n",
    "        games g\n",
    "    JOIN\n",
    "        game_embs ge ON g.id = ge.game_id\n",
    "    WHERE\n",
    "        g.booth_number IS NOT NULL -- Ensure we only sort valid numbers\n",
    "    ORDER BY\n",
    "        g.booth_number DESC\n",
    "    LIMIT 3;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Process the results\n",
    "    for row in rows:\n",
    "        game_id, name, booth_number, embedding_blob = row\n",
    "\n",
    "        # Deserialize the embedding blob\n",
    "        # The format string '<{embedding_dim}f' means:\n",
    "        # '<' = little-endian\n",
    "        # '{embedding_dim}' = number of floats (e.g., 1536)\n",
    "        # 'f' = float (4 bytes)\n",
    "        try:\n",
    "            # Use struct.unpack to convert bytes back to tuple of floats\n",
    "            embedding_tuple = struct.unpack(f\"<{embedding_dim}f\", embedding_blob)\n",
    "            # Convert tuple to list\n",
    "            embedding_list = list(embedding_tuple)\n",
    "        except struct.error as e:\n",
    "            print(f\"Error unpacking embedding for game {game_id}: {e}\")\n",
    "            print(\n",
    "                f\"Blob length: {len(embedding_blob)} bytes. Expected: {embedding_dim * 4} bytes.\"\n",
    "            )\n",
    "            embedding_list = None  # Indicate failure\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"id\": game_id,\n",
    "                \"name\": name,\n",
    "                \"booth_number\": booth_number,\n",
    "                \"embedding\": embedding_list,  # Now it's a list of floats\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Print the results\n",
    "for game in results:\n",
    "    # Print embedding length or first few elements for brevity\n",
    "    emb_preview = game[\"embedding\"][:5] if game[\"embedding\"] else \"Error/None\"\n",
    "    print(f\"Game ID: {game['id']}\")\n",
    "    print(f\"Name: {game['name']}\")\n",
    "    print(f\"Booth #: {game['booth_number']}\")\n",
    "    print(f\"Embedding (first 5): {emb_preview}...\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
