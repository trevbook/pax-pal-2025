{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing SQLite Database**\n",
    "After doing a bunch of data preparation in the previous notebooks, I'm ready to create a SQLite database with all of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook. \n",
    "\n",
    "I'll start by configuring the kernel that's running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the cwd\n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "\n",
    "# Project-specific imports \n",
    "import utils.openai as openai_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "First off: I'll load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the playable_games_df\n",
    "playable_games_df = pd.read_json(\"data/final_enriched_games_data.json\")\n",
    "\n",
    "# If any rows have an empty `description_texts` / `genres_and_tags`, drop them\n",
    "playable_games_df = playable_games_df[\n",
    "    playable_games_df[\"description_texts\"].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0\n",
    "    )\n",
    "    & playable_games_df[\"genres_and_tags\"].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many games do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(playable_games_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Text\n",
    "Before I actually create the SQLite database, I'm going to embed all of the relevant text. Specifically, I'll embed: \n",
    "\n",
    "- The average of each of the texts within `description_texts`\n",
    "- The `snappy_summary`\n",
    "- A comma-separated string of everything in `genres_and_tags`\n",
    "\n",
    "I'll start by grabbing all of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text_to_embed_df\n",
    "text_to_embed_df_records = []\n",
    "for row in playable_games_df.itertuples():\n",
    "    # Find the longest description text\n",
    "    longest_description = max(\n",
    "        [desc_dict.get(\"text\", \"\") for desc_dict in row.description_texts],\n",
    "        key=len,\n",
    "        default=\"\",\n",
    "    )\n",
    "\n",
    "    # Create a single combined text\n",
    "    combined_text = f\"{row.name} {row.snappy_summary} {', '.join(row.genres_and_tags)} {longest_description}\"\n",
    "\n",
    "    text_to_embed_df_records.append(\n",
    "        {\"game_id\": row.id, \"emb_type\": \"combined\", \"text\": combined_text}\n",
    "    )\n",
    "text_to_embed_df = pd.DataFrame(text_to_embed_df_records).dropna(subset=[\"text\"])\n",
    "\n",
    "# Drop anything where the text is not a string\n",
    "text_to_embed_df = text_to_embed_df[\n",
    "    text_to_embed_df[\"text\"].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll embed everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = openai_utils.generate_embeddings_for_texts(\n",
    "    text_list=text_to_embed_df[\"text\"].tolist(), show_progress=True\n",
    ")\n",
    "\n",
    "# Add the embeddings to the dataframe\n",
    "embs_df = text_to_embed_df.copy()\n",
    "embs_df[\"emb\"] = embs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating SQLite Database\n",
    "Next up: I'm going to save all of the data in a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import sqlite_vec  # vec0 helper\n",
    "import time\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Helper â”€ execute_many with retry/back-off to dodge the \"database is locked\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _execute_many_with_retry(\n",
    "    cursor,\n",
    "    sql: str,\n",
    "    data,\n",
    "    *,\n",
    "    max_attempts: int = 5,\n",
    "    initial_wait: float = 0.3,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run `cursor.executemany(sql, data)` with exponential back-off so that the\n",
    "    notebook doesn't crash when another process (or stray connection from a\n",
    "    previous cell) is holding a write-lock on the SQLite file.\n",
    "\n",
    "    Any non-locking OperationalError is re-raised immediately.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            cursor.executemany(sql, data)\n",
    "            return  # âœ… success\n",
    "        except sqlite3.OperationalError as exc:\n",
    "            # Only retry if it's really a lock issue\n",
    "            msg = str(exc).lower()\n",
    "            if \"database is locked\" not in msg:\n",
    "                raise\n",
    "\n",
    "            wait_time = initial_wait * (2**attempt)\n",
    "            print(\n",
    "                f\"SQLite is locked; retrying in {wait_time:.2f}s ({attempt+1}/{max_attempts})\"\n",
    "            )\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    # all retries exhausted\n",
    "    raise RuntimeError(\n",
    "        f\"Could not run query after {max_attempts} attempts; still locked.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Connection set-up\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "db_path = \"data/database.sqlite\"\n",
    "with sqlite3.connect(\n",
    "    db_path, timeout=30.0, isolation_level=None, check_same_thread=False\n",
    ") as conn:\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)  # load vec0 extension\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Use default journal mode instead of WAL\n",
    "    cursor.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Schema\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cursor.executescript(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS games (\n",
    "            id                TEXT PRIMARY KEY,\n",
    "            name              TEXT,\n",
    "            snappy_summary    TEXT,\n",
    "            description_texts TEXT,\n",
    "            platforms         TEXT,\n",
    "            developer         TEXT,\n",
    "            exhibitor         TEXT,\n",
    "            booth_number      REAL,\n",
    "            header_image_url  TEXT,\n",
    "            steam_link        TEXT,\n",
    "            genres_and_tags   TEXT,\n",
    "            media             TEXT,\n",
    "            released          REAL,\n",
    "            release_time      TEXT,\n",
    "            links             TEXT\n",
    "        );\n",
    "\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS game_embs\n",
    "        USING vec0(\n",
    "            game_id TEXT PRIMARY KEY,\n",
    "            vector  FLOAT[1536]\n",
    "        );\n",
    "        \n",
    "        -- Full-text search index on human-readable text\n",
    "        CREATE VIRTUAL TABLE IF NOT EXISTS games_fts\n",
    "        USING fts5(\n",
    "            id UNINDEXED,\n",
    "            text\n",
    "        );\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1. Upsert games\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    game_records = [\n",
    "        (\n",
    "            row.id,\n",
    "            row.name,\n",
    "            getattr(row, \"snappy_summary\", None),\n",
    "            json.dumps(row.description_texts),\n",
    "            json.dumps(row.platforms),\n",
    "            row.developer,\n",
    "            row.exhibitor,\n",
    "            row.booth_number,\n",
    "            row.header_image_url,\n",
    "            row.steam_link,\n",
    "            json.dumps(row.genres_and_tags),\n",
    "            json.dumps(row.media),\n",
    "            row.released,\n",
    "            row.release_time,\n",
    "            json.dumps(getattr(row, \"links\", None)),\n",
    "        )\n",
    "        for row in playable_games_df.itertuples()\n",
    "    ]\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO games (\n",
    "            id, name, snappy_summary, description_texts, platforms,\n",
    "            developer, exhibitor, booth_number, header_image_url, steam_link,\n",
    "            genres_and_tags, media, released, release_time, links\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        game_records,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2. Upsert embeddings (avoid UNIQUE constraint conflicts)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # clear out any stale rows\n",
    "    cursor.execute(\"DELETE FROM game_embs\")\n",
    "\n",
    "    # drop any accidental duplicates in the DataFrame\n",
    "    deduped = embs_df.drop_duplicates(subset=\"game_id\", keep=\"last\")\n",
    "    emb_records = [\n",
    "        (row[\"game_id\"], json.dumps(row[\"emb\"])) for _, row in deduped.iterrows()\n",
    "    ]\n",
    "\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"INSERT OR REPLACE INTO game_embs (game_id, vector) VALUES (?, ?)\",\n",
    "        emb_records,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3. Populate full-text search index\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cursor.execute(\"DELETE FROM games_fts\")\n",
    "\n",
    "    # Prepare records for FTS table\n",
    "    fts_records = []\n",
    "    for _, row in embs_df.iterrows():\n",
    "        fts_records.append((row[\"game_id\"], row[\"text\"]))\n",
    "\n",
    "    _execute_many_with_retry(\n",
    "        cursor,\n",
    "        \"\"\"\n",
    "        INSERT INTO games_fts (id, text)\n",
    "        VALUES (?, ?)\n",
    "        \"\"\",\n",
    "        fts_records,\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Database created successfully with \"\n",
    "    f\"{len(playable_games_df)} games and {len(embs_df)} embeddings ðŸŽ®\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec\n",
    "import struct\n",
    "import json  # For potentially pretty-printing game details if needed\n",
    "\n",
    "# Database path\n",
    "db_path = \"data/database.sqlite\"\n",
    "\n",
    "# Dimension of your embeddings (must match what you stored)\n",
    "# From your schema: CREATE VIRTUAL TABLE ... vector FLOAT[1536]\n",
    "embedding_dim = 1536\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Connect to the database\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    # Enable extension loading and load sqlite-vec\n",
    "    # Although not strictly needed for reading blobs,\n",
    "    # it's good practice if you might do vector operations later.\n",
    "    conn.enable_load_extension(True)\n",
    "    sqlite_vec.load(conn)\n",
    "    conn.enable_load_extension(False)  # Disable after loading\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Query to get top 3 games by booth number (desc) and their embeddings\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        g.id,\n",
    "        g.name,\n",
    "        g.booth_number,\n",
    "        ge.vector  -- This will be a BLOB\n",
    "    FROM\n",
    "        games g\n",
    "    JOIN\n",
    "        game_embs ge ON g.id = ge.game_id\n",
    "    WHERE\n",
    "        g.booth_number IS NOT NULL -- Ensure we only sort valid numbers\n",
    "    ORDER BY\n",
    "        g.booth_number DESC\n",
    "    LIMIT 3;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Process the results\n",
    "    for row in rows:\n",
    "        game_id, name, booth_number, embedding_blob = row\n",
    "\n",
    "        # Deserialize the embedding blob\n",
    "        # The format string '<{embedding_dim}f' means:\n",
    "        # '<' = little-endian\n",
    "        # '{embedding_dim}' = number of floats (e.g., 1536)\n",
    "        # 'f' = float (4 bytes)\n",
    "        try:\n",
    "            # Use struct.unpack to convert bytes back to tuple of floats\n",
    "            embedding_tuple = struct.unpack(f\"<{embedding_dim}f\", embedding_blob)\n",
    "            # Convert tuple to list\n",
    "            embedding_list = list(embedding_tuple)\n",
    "        except struct.error as e:\n",
    "            print(f\"Error unpacking embedding for game {game_id}: {e}\")\n",
    "            print(\n",
    "                f\"Blob length: {len(embedding_blob)} bytes. Expected: {embedding_dim * 4} bytes.\"\n",
    "            )\n",
    "            embedding_list = None  # Indicate failure\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"id\": game_id,\n",
    "                \"name\": name,\n",
    "                \"booth_number\": booth_number,\n",
    "                \"embedding\": embedding_list,  # Now it's a list of floats\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Print the results\n",
    "for game in results:\n",
    "    # Print embedding length or first few elements for brevity\n",
    "    emb_preview = game[\"embedding\"][:5] if game[\"embedding\"] else \"Error/None\"\n",
    "    print(f\"Game ID: {game['id']}\")\n",
    "    print(f\"Name: {game['name']}\")\n",
    "    print(f\"Booth #: {game['booth_number']}\")\n",
    "    print(f\"Embedding (first 5): {emb_preview}...\")\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
