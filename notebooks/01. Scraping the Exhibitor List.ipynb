{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scraping the Exhibitor List**\n",
    "PAX just dropped [the 2025 exhibitor list](https://east.paxsite.com/en-us/expo-hall.html); within this notebook, I'll scrape it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook. \n",
    "\n",
    "I'll start by configuring the kernel that's running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the cwd\n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import random\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "# Third-party imports\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Exhibitor List\n",
    "The Exhibitor List is actually loaded dynamically via JavaScript after the initial page load. Because of this, I've just used the Chrome inspector to copy the HTML data *after* the load; I can scrape from this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML from the saved file\n",
    "with open(\"data/full-exhibitor-list-html.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "# Define a method to scrape the exhibitors\n",
    "def scrape_exhibitors(html_content):\n",
    "    \"\"\"\n",
    "    Extract exhibitor information from the PAX East exhibitor list HTML.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): The HTML content of the exhibitor list page\n",
    "\n",
    "    Returns:\n",
    "        exhibitor_list (List[dict]): A list of dictionaries containing exhibitor information\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    exhibitors = []\n",
    "\n",
    "    # Find all exhibitor entries\n",
    "    exhibitor_entries = soup.find_all(\"div\", class_=\"exhibitor-entry\")\n",
    "\n",
    "    for entry in exhibitor_entries:\n",
    "        exhibitor = {}\n",
    "\n",
    "        # Get image URL\n",
    "        img_tag = entry.find(\"img\")\n",
    "        exhibitor[\"image_url\"] = img_tag[\"src\"] if img_tag else None\n",
    "\n",
    "        # Get name\n",
    "        name_tag = entry.find(\"div\", class_=\"exhibitor-name\").find(\"a\")\n",
    "        exhibitor[\"name\"] = name_tag.text.strip() if name_tag else None\n",
    "\n",
    "        # Get description\n",
    "        desc_tag = entry.find(\"div\", class_=\"exhibitor-description\")\n",
    "        exhibitor[\"description_excerpt\"] = desc_tag.text.strip() if desc_tag else None\n",
    "\n",
    "        # Get booths\n",
    "        location_tag = entry.find(\"div\", class_=\"exhibitor-location\")\n",
    "        booth_text = location_tag.text if location_tag else \"\"\n",
    "        # Extract booth numbers using regex\n",
    "        booth_numbers = re.findall(r\"\\d+\", booth_text)\n",
    "        exhibitor[\"booths\"] = booth_numbers\n",
    "\n",
    "        # Get details page link\n",
    "        link_tag = entry.find(\"a\", class_=\"gtExhibitorLink\")\n",
    "        if link_tag:\n",
    "            # Fix the URL by removing the duplicate domain\n",
    "            href = link_tag[\"href\"]\n",
    "            if href.startswith(\"https://east.paxsite.com\"):\n",
    "                exhibitor[\"details_url\"] = href\n",
    "            else:\n",
    "                exhibitor[\"details_url\"] = f\"https://east.paxsite.com{href}\"\n",
    "        else:\n",
    "            exhibitor[\"details_url\"] = None\n",
    "\n",
    "        exhibitors.append(exhibitor)\n",
    "\n",
    "    return exhibitors\n",
    "\n",
    "\n",
    "# Call the function to scrape exhibitors\n",
    "exhibitors = scrape_exhibitors(html_content)\n",
    "\n",
    "# Convert the list of exhibitors to a DataFrame\n",
    "exhibitor_df = pd.DataFrame(exhibitors)\n",
    "\n",
    "# Add a column for the number of booths\n",
    "exhibitor_df[\"n_booths\"] = exhibitor_df[\"booths\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this scraped data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitor_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Exhibitor Details\n",
    "Next up: for each of the exhibitors on the list, I'm going to scrape some details related to them.\n",
    "\n",
    "The method below will use a headless Playwright client to scrape the details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_exhibitor_details(url):\n",
    "    from tenacity import (\n",
    "        retry,\n",
    "        stop_after_attempt,\n",
    "        wait_exponential,\n",
    "        retry_if_exception_type,\n",
    "    )\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "        retry=retry_if_exception_type((TimeoutError, Exception)),\n",
    "        reraise=True,\n",
    "    )\n",
    "    async def _scrape_with_retry(page, url):\n",
    "        # Navigate to the URL with a longer timeout\n",
    "        await page.goto(url, timeout=30000, wait_until=\"networkidle\")\n",
    "\n",
    "        # Wait for the main content to load with a longer timeout\n",
    "        await page.wait_for_selector(\".innovation__base\", timeout=20000)\n",
    "\n",
    "        # Add a small delay to ensure JavaScript has fully executed\n",
    "        await page.wait_for_timeout(random.uniform(1000, 2000))\n",
    "\n",
    "        # Create a result dictionary with default values\n",
    "        result = {\n",
    "            \"details_url\": url,\n",
    "            \"booth\": None,\n",
    "            \"name\": None,\n",
    "            \"description\": None,\n",
    "            \"website_url\": None,\n",
    "            \"shop_url\": None,\n",
    "            \"playable_games\": [],\n",
    "        }\n",
    "\n",
    "        # Extracting the booth number\n",
    "        booth_element = await page.query_selector(\"h3.showroomLocation\")\n",
    "        if booth_element:\n",
    "            booth_text = await booth_element.inner_text()\n",
    "            # Extract just the number part after \"Booth:\"\n",
    "            if \":\" in booth_text:\n",
    "                result[\"booth\"] = booth_text.split(\":\", 1)[1].strip()\n",
    "\n",
    "        # Extracting the name from the logo alt text\n",
    "        logo_element = await page.query_selector(\"#showroomLogo img\")\n",
    "        if logo_element:\n",
    "            result[\"name\"] = await logo_element.get_attribute(\"alt\")\n",
    "        # Fallback to data attribute if available\n",
    "        if not result[\"name\"]:\n",
    "            logo_link = await page.query_selector(\".showroom-logo-modal\")\n",
    "            if logo_link:\n",
    "                result[\"name\"] = await logo_link.get_attribute(\"data-exhib-full-name\")\n",
    "\n",
    "        # Extracting the description\n",
    "        desc_element = await page.query_selector(\"div#showroomDesc p\")\n",
    "        if desc_element:\n",
    "            result[\"description\"] = await desc_element.inner_text()\n",
    "\n",
    "        # Extracting the website URL\n",
    "        website_element = await page.query_selector(\"li.website a\")\n",
    "        if website_element:\n",
    "            result[\"website_url\"] = await website_element.get_attribute(\"href\")\n",
    "\n",
    "        # Extracting the shop URL\n",
    "        shop_element = await page.query_selector(\"li.store a\")\n",
    "        if shop_element:\n",
    "            result[\"shop_url\"] = await shop_element.get_attribute(\"href\")\n",
    "\n",
    "        # Extracting playable games\n",
    "        game_elements = await page.query_selector_all(\n",
    "            \"ul.singleSpecialList li.singleSpecial\"\n",
    "        )\n",
    "        for game_element in game_elements:\n",
    "            game_data = {}\n",
    "\n",
    "            title_element = await game_element.query_selector(\n",
    "                \"div.gtSlideSpecial-title\"\n",
    "            )\n",
    "            if title_element:\n",
    "                game_data[\"name\"] = await title_element.inner_text()\n",
    "            else:\n",
    "                game_data[\"name\"] = None\n",
    "\n",
    "            image_element = await game_element.query_selector(\n",
    "                \"div.gtSlideSpecial-image\"\n",
    "            )\n",
    "            if image_element:\n",
    "                image_style = await image_element.get_attribute(\"style\")\n",
    "                if image_style and \"url(\" in image_style:\n",
    "                    url_part = image_style.split(\"url(\")[1].split(\")\")[0]\n",
    "                    game_data[\"image_url\"] = url_part.strip(\"'\\\"\")\n",
    "                else:\n",
    "                    game_data[\"image_url\"] = None\n",
    "            else:\n",
    "                game_data[\"image_url\"] = None\n",
    "\n",
    "            link_element = await game_element.query_selector(\"a\")\n",
    "            if link_element:\n",
    "                game_data[\"merch_link\"] = await link_element.get_attribute(\n",
    "                    \"data-backupurl\"\n",
    "                )\n",
    "            else:\n",
    "                game_data[\"merch_link\"] = None\n",
    "\n",
    "            result[\"playable_games\"].append(game_data)\n",
    "\n",
    "        return result\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        try:\n",
    "            result = await _scrape_with_retry(page, url)\n",
    "        except Exception:\n",
    "            # If all retries fail, return empty result with URL\n",
    "            result = {\n",
    "                \"details_url\": url,\n",
    "                \"booth\": None,\n",
    "                \"name\": None,\n",
    "                \"description\": None,\n",
    "                \"website_url\": None,\n",
    "                \"shop_url\": None,\n",
    "                \"playable_games\": [],\n",
    "            }\n",
    "\n",
    "        await browser.close()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the exhibitors I've found, I'll scrape their details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitor_details_list = []\n",
    "for row in tqdm(\n",
    "    iterable=list(exhibitor_df.itertuples()),\n",
    "    desc=\"Scraping exhibitor details\",\n",
    "):\n",
    "    # Skip if the URL is None\n",
    "    if row.details_url is None:\n",
    "        continue\n",
    "\n",
    "    # Otherwise, scrape the details\n",
    "    exhibitor_details_list.append(await scrape_exhibitor_details(row.details_url))\n",
    "\n",
    "    # Sleep for a random amount of time\n",
    "    sleep_time = random.uniform(3, 8)\n",
    "    sleep(sleep_time)\n",
    "\n",
    "# Make a DataFrame from the list of dictionaries\n",
    "exhibitor_details_df = pd.DataFrame(exhibitor_details_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibitor_details_df.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
